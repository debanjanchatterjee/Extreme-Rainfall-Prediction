{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\deban\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = pd.read_csv('X_trainData_1.csv')\n",
    "#labels=pd.read_csv('Y_trainData_1.csv')\n",
    "dataset = pd.read_csv('X_24hrs.csv')\n",
    "datalabels = pd.read_csv('y_mumbai.csv')\n",
    "#datalabels=pd.read_csv('2005-2008_yvalue_mumbai.csv')\n",
    "\n",
    "\n",
    "#X_train=dataset.iloc[:,:].values\n",
    "\n",
    "X=dataset.iloc[:,1:].values\n",
    "y=datalabels.iloc[:,1:].values\n",
    "#y_train=datalabels.iloc[:,-1].values\n",
    "#y=datalabels.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2440, 9900)\n",
      "[[300.87     292.05     276.39     ...  20.        28.75      35.25    ]\n",
      " [300.47     291.72     275.92     ...  28.25      70.250015  59.000015]\n",
      " [300.99     290.92     276.07     ...  26.75      65.500015  47.750015]\n",
      " ...\n",
      " [300.07498  290.55     275.35     ...  46.5       61.5       48.25    ]\n",
      " [300.14996  290.9      275.575    ...  47.25      62.        22.      ]\n",
      " [300.5      290.8      275.77496  ...  32.75      66.5       19.      ]]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2440, 1)\n",
      "[[ 0.        ]\n",
      " [19.535807  ]\n",
      " [ 0.        ]\n",
      " ...\n",
      " [ 1.0364816 ]\n",
      " [ 0.18965113]\n",
      " [ 0.18965113]]\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.48259008901639\n",
      "29.95223440768119\n"
     ]
    }
   ],
   "source": [
    "print(y.mean())\n",
    "print(y.std())\n",
    "mean_rf=y.mean()\n",
    "std_rf=y.std()\n",
    "thr=mean_rf+std_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in range(0, len(y)):\n",
    "\n",
    "  if y[i]>thr:\n",
    "    y[i]=1\n",
    "    count=count+1\n",
    "  else:\n",
    "    y[i]=0\n",
    "print(count)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102920.0\n"
     ]
    }
   ],
   "source": [
    "print(X.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "X[:, :] = sc.fit_transform(X[:, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the array of the first training input looks like [ 0.77220409  1.31421608  1.26477996 ... -1.02416529 -0.92323991\n",
      " -0.26355716]\n",
      "WARNING:tensorflow:From C:\\Users\\deban\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\training\\adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch 0 / 0 loss: 12716.81069946289\n"
     ]
    }
   ],
   "source": [
    "all_data = X\n",
    "#looking at the shape of the file\n",
    "num_rows,num_cols= all_data.shape\n",
    "\n",
    "# printing the array representation of one entry in csv file\n",
    "print(\"the array of the first training input looks like\", all_data[0])\n",
    "\n",
    "\n",
    "# Deciding how many nodes wach layer should have\n",
    "n_nodes_inpl = num_cols  #encoder\n",
    "n_nodes_hl1  = 2500  #encoder\n",
    "n_nodes_hl2  = 2500  #decoder\n",
    "n_nodes_outl = num_cols  #decoder\n",
    "\n",
    "# first hidden layer has 9900*2500 weights and 2500 biases\n",
    "hidden_1_layer_vals = {\n",
    "'weights':tf.Variable(tf.random.normal([n_nodes_inpl,n_nodes_hl1])),\n",
    "'biases':tf.Variable(tf.random.normal([n_nodes_hl1]))  }\n",
    "# second hidden layer has 2500*2500 weights and 2500 biases\n",
    "hidden_2_layer_vals = {\n",
    "'weights':tf.Variable(tf.random.normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "'biases':tf.Variable(tf.random.normal([n_nodes_hl2]))  }\n",
    "# second hidden layer has 2500*9900 weights and 9900 biases\n",
    "output_layer_vals = {\n",
    "'weights':tf.Variable(tf.random.normal([n_nodes_hl2,n_nodes_outl])), 'biases':tf.Variable(tf.random.normal([n_nodes_outl])) }\n",
    "\n",
    "# training sample goes in\n",
    "input_layer = tf.placeholder('float', [None, num_cols])\n",
    "# multiply output of input_layer wth a weight matrix and add biases\n",
    "layer_1 = tf.nn.sigmoid(\n",
    "       tf.add(tf.matmul(input_layer,hidden_1_layer_vals['weights']),\n",
    "       hidden_1_layer_vals['biases']))\n",
    "# multiply output of layer_1 wth a weight matrix and add biases\n",
    "layer_2 = tf.nn.sigmoid(\n",
    "       tf.add(tf.matmul(layer_1,hidden_2_layer_vals['weights']),\n",
    "       hidden_2_layer_vals['biases']))\n",
    "# multiply output of layer_2 wth a weight matrix and add biases\n",
    "output_layer = tf.matmul(layer_2,output_layer_vals['weights']) +\\\n",
    "               output_layer_vals['biases']\n",
    "# output_true shall have the original training sample for error calculations\n",
    "output_true = tf.placeholder('float', [None, num_cols])\n",
    "# define our cost function\n",
    "meansq =    tf.reduce_mean(tf.square(output_layer - output_true))\n",
    "# define our optimizer\n",
    "learn_rate = 0.1   # how fast the model should learn\n",
    "optimizer = tf.train.AdagradOptimizer(learn_rate).minimize(meansq)\n",
    "\n",
    "\n",
    "# initialising stuff and starting the session\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "# defining batch size, number of epochs and learning rate\n",
    "batch_size = 100  # how many samples to use together for training\n",
    "hm_epochs = 1    # how many times to go through the entire dataset\n",
    "tot_samples = 2440 # total number of samples\n",
    "\n",
    "# total improvement is printed out after each epoch\n",
    "for epoch in range(hm_epochs):\n",
    "    epoch_loss = 0    # initializing error as 0\n",
    "    for i in range(int(tot_samples/batch_size)):\n",
    "        epoch_x = all_data[ i*batch_size : (i+1)*batch_size ]\n",
    "        _, c = sess.run([optimizer, meansq],\\\n",
    "               feed_dict={input_layer: epoch_x, \\\n",
    "               output_true: epoch_x})\n",
    "        epoch_loss += c\n",
    "    print('Epoch', epoch, '/', hm_epochs-1, 'loss:',epoch_loss)\n",
    "\n",
    "# running it though just the encoder\n",
    "#df_comp = pd.DataFrame()\n",
    "i=0\n",
    "arr = np.empty((0,n_nodes_hl1), int)\n",
    "for sample in all_data:\n",
    "  encoded_sample = sess.run(layer_1,\\\n",
    "                   feed_dict={input_layer:[sample]})\n",
    "  arr = np.append(arr, encoded_sample, axis=0)\n",
    "\n",
    "# print the size of encoded data\n",
    "print(\"size of encoded data set is:\")  \n",
    "print(arr.shape)\n",
    " \n",
    "X=arr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_normal.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train[:, :] = sc.fit_transform(X_train[:, :])\n",
    "X_test[:, :] = sc.transform(X_test[:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "#classifier = XGBClassifier(scale_pos_weight=20)\n",
    "classifier = XGBClassifier(scale_pos_weight=10)\n",
    "classifier.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precision=cm[1][1]/(cm[1][1]+cm[0][1])\n",
    "#recall=cm[1][1]/(cm[1][1]+cm[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "print('Accuracy- '+str(accuracy_score(y_test, y_pred)))\n",
    "print('Precision- '+str(precision_score(y_test,y_pred)))\n",
    "print('Recall- '+str(recall_score(y_test, y_pred)))\n",
    "print(\"ROC_AUC score- \"+str(roc_auc_score(y_test, y_pred)))\n",
    "print('F1-score- '+str(f1_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "\n",
    "df = pd.DataFrame(cm, index = ['Non Extreme Rainfall', 'Extreme  Rainfall'], columns = ['Non Extreme Rainfall', 'Extreme  Rainfall'])\n",
    "\n",
    "# Plot the heatmap\n",
    "sn.heatmap(df, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
