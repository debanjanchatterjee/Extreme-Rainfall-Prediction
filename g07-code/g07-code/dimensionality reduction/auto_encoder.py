# -*- coding: utf-8 -*-
"""auto_encoder

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BeLkBxFEEd5A_0aFQI_EecUXMXZKC5ak
"""

# Importing tensorflow
#import tensorflow as tf
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import csv
#loading the X values
## change the input path accordingly:
df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/combined_gridview_data_1997_2000.csv',\
                  delimiter=',')
df.drop('time',inplace=True,axis=1)
df.drop('past_day1_time',inplace=True,axis=1)
df.drop('past_day2_time',inplace=True,axis=1)

all_data = np.array(df)
#looking at the shape of the file
num_rows,num_cols= all_data.shape

# printing the array representation of one entry in csv file
print("the array of the first training input looks like", all_data[0])


# Deciding how many nodes wach layer should have
n_nodes_inpl = num_cols  #encoder
n_nodes_hl1  = 2500  #encoder
n_nodes_hl2  = 2500  #decoder
n_nodes_outl = num_cols  #decoder

# first hidden layer has 9900*2500 weights and 2500 biases
hidden_1_layer_vals = {
'weights':tf.Variable(tf.random.normal([n_nodes_inpl,n_nodes_hl1])),
'biases':tf.Variable(tf.random.normal([n_nodes_hl1]))  }
# second hidden layer has 2500*2500 weights and 2500 biases
hidden_2_layer_vals = {
'weights':tf.Variable(tf.random.normal([n_nodes_hl1, n_nodes_hl2])),
'biases':tf.Variable(tf.random.normal([n_nodes_hl2]))  }
# second hidden layer has 2500*9900 weights and 9900 biases
output_layer_vals = {
'weights':tf.Variable(tf.random.normal([n_nodes_hl2,n_nodes_outl])), 'biases':tf.Variable(tf.random.normal([n_nodes_outl])) }

# training sample goes in
input_layer = tf.placeholder('float', [None, num_cols])
# multiply output of input_layer wth a weight matrix and add biases
layer_1 = tf.nn.sigmoid(
       tf.add(tf.matmul(input_layer,hidden_1_layer_vals['weights']),
       hidden_1_layer_vals['biases']))
# multiply output of layer_1 wth a weight matrix and add biases
layer_2 = tf.nn.sigmoid(
       tf.add(tf.matmul(layer_1,hidden_2_layer_vals['weights']),
       hidden_2_layer_vals['biases']))
# multiply output of layer_2 wth a weight matrix and add biases
output_layer = tf.matmul(layer_2,output_layer_vals['weights']) +\
               output_layer_vals['biases']
# output_true shall have the original training sample for error calculations
output_true = tf.placeholder('float', [None, num_cols])
# define our cost function
meansq =    tf.reduce_mean(tf.square(output_layer - output_true))
# define our optimizer
learn_rate = 0.1   # how fast the model should learn
optimizer = tf.train.AdagradOptimizer(learn_rate).minimize(meansq)


# initialising stuff and starting the session
init = tf.global_variables_initializer()
sess = tf.Session()
sess.run(init)
# defining batch size, number of epochs and learning rate
batch_size = 100  # how many samples to use together for training
hm_epochs = 1    # how many times to go through the entire dataset
tot_samples = 488 # total number of samples

# total improvement is printed out after each epoch
for epoch in range(hm_epochs):
    epoch_loss = 0    # initializing error as 0
    for i in range(int(tot_samples/batch_size)):
        epoch_x = all_data[ i*batch_size : (i+1)*batch_size ]
        _, c = sess.run([optimizer, meansq],\
               feed_dict={input_layer: epoch_x, \
               output_true: epoch_x})
        epoch_loss += c
    print('Epoch', epoch, '/', hm_epochs-1, 'loss:',epoch_loss)

# running it though just the encoder
#df_comp = pd.DataFrame()
i=0
arr = np.empty((0,n_nodes_hl1), int)
for sample in all_data:
  encoded_sample = sess.run(layer_1,\
                   feed_dict={input_layer:[sample]})
  arr = np.append(arr, encoded_sample, axis=0)

# print the size of encoded data
print("size of encoded data set is:")  
print(arr.shape)
df_out = pd.DataFrame(arr)
## change the output path accordingly:##
df_out.to_csv("./drive/MyDrive/compressed.csv",index=False,header=False)

